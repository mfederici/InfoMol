{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:10:31.133135021Z",
     "start_time": "2024-02-28T13:10:29.781060978Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from typing import Tuple, Optional, List, Union\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "import tables\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CachedTensorDict:\n",
    "    DATASET_NAME = 'data'\n",
    "    TENSOR_FILE = 'values.h5py'\n",
    "    INDEX_FILE = 'index.pkl'\n",
    "    def __init__(self, path: str, name:str, shape: Tuple[int, ...], write_size: int = 1000, read_only: bool = False):\n",
    "        assert os.path.isdir(path)\n",
    "\n",
    "        full_path = os.path.join(path, name)\n",
    "        if not os.path.isdir(full_path):\n",
    "            os.mkdir(full_path)\n",
    "\n",
    "        self.path = full_path\n",
    "        self.tensor_file = os.path.join(full_path, self.TENSOR_FILE)\n",
    "        self.index_file = os.path.join(full_path, self.INDEX_FILE)\n",
    "\n",
    "        self.write_size = write_size\n",
    "        self.read_only = read_only\n",
    "        self._base_shape = shape\n",
    "\n",
    "        print(f\"Opening {self.tensor_file}\")\n",
    "\n",
    "        try:\n",
    "            if self.read_only:\n",
    "                f = h5py.File(self.tensor_file, 'r')\n",
    "            else:\n",
    "                f = h5py.File(self.tensor_file, 'a')\n",
    "                if not self.DATASET_NAME in f:\n",
    "                    f.create_dataset(\n",
    "                        self.DATASET_NAME,\n",
    "                        shape=(0, *shape),\n",
    "                        maxshape=(None, *shape)\n",
    "                    )\n",
    "                if not os.path.isfile(self.index_file):\n",
    "                    with open(self.index_file, 'wb') as file:\n",
    "                        pickle.dump({}, file)\n",
    "        except BlockingIOError as e:\n",
    "            print(f\"Unable to open {self.tensor_file}\")\n",
    "            raise e\n",
    "\n",
    "        assert self.DATASET_NAME in f\n",
    "        self._cache = f[self.DATASET_NAME]\n",
    "\n",
    "        with open(self.index_file, 'rb') as file:\n",
    "            self._index = pickle.load(file)\n",
    "\n",
    "        self._new_cache = {}\n",
    "        self._f = f\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int, ...]:\n",
    "        return (len(self._index), *self._base_shape)\n",
    "\n",
    "    def write_cache_to_disk(self):\n",
    "        assert not self.read_only\n",
    "\n",
    "        if len(self._new_cache) == 0:\n",
    "            return\n",
    "\n",
    "        # Write the freshly added data\n",
    "        old_cache_size = self._cache.shape[0]\n",
    "        new_cache_size = len(self._index)\n",
    "\n",
    "        # Resize to fit the new data\n",
    "        self._cache.resize((new_cache_size, *self._base_shape))\n",
    "\n",
    "        # Write the new values\n",
    "        for smile, value in self._new_cache.items():\n",
    "            # Empty entry\n",
    "            if not value is None:\n",
    "                idx = self._index[smile]\n",
    "                assert idx >= old_cache_size, f\"{idx}, {old_cache_size}\"\n",
    "                self._cache[idx] = value\n",
    "\n",
    "        # Update the index\n",
    "        with open(self.index_file, 'wb') as f:\n",
    "            pickle.dump(self._index, f)\n",
    "\n",
    "        self._new_cache = {}\n",
    "\n",
    "    def _add_to_cache(self, key: str, value: np.ndarray):\n",
    "        assert not self.read_only\n",
    "        assert not (key in self._index) and not (key in self._new_cache)\n",
    "\n",
    "        self._index[key] = len(self._index)\n",
    "        self._new_cache[key] = value\n",
    "\n",
    "        # Write to disk when the data in memory is too much\n",
    "        if len(self._index)-self._cache.shape[0] >= self.write_size:\n",
    "            self.write_cache_to_disk()\n",
    "\n",
    "\n",
    "    def __contains__(self, key: str):\n",
    "        return key in self._index or key in self._new_cache\n",
    "\n",
    "    def __getitem__(self, key: Union[str, List[str]]) -> np.ndarray:\n",
    "        if isinstance(key, str):\n",
    "            if key in self._new_cache:\n",
    "                value = self._new_cache[key]\n",
    "            else:\n",
    "                idx = self._index[key]\n",
    "                value = self._cache[idx]\n",
    "            return value\n",
    "        elif isinstance(key, list):\n",
    "            # Write everything to cache\n",
    "            if not self.read_only:\n",
    "                self.write_cache_to_disk()\n",
    "\n",
    "            # Then look up\n",
    "            ids = np.array([self._index[k] for k in key])\n",
    "\n",
    "            # Make sure the ids are ordered (h5py supports only ordered access)\n",
    "            if np.all(ids[:-1] < ids[1:]):\n",
    "                values = self._cache[ids]\n",
    "\n",
    "            # If the ids are not in order\n",
    "            else:\n",
    "                # Step 1: Sort the IDs and remember the original indices\n",
    "                id_order = np.argsort(ids)\n",
    "                sorted_ids = ids[id_order]\n",
    "\n",
    "                # Step 2: Check for duplicate ids and remove them from the query\n",
    "                duplicates = not np.all(sorted_ids[:-1] < sorted_ids[1:])\n",
    "                if duplicates:\n",
    "                    non_duplicate_mask = np.concatenate(\n",
    "                        [np.array([True]).reshape(1), sorted_ids[:-1] < sorted_ids[1:]], 0\n",
    "                    )\n",
    "                    sorted_ids = sorted_ids[non_duplicate_mask]\n",
    "                    id_lookup = np.cumsum(non_duplicate_mask) - 1\n",
    "                else:\n",
    "                    id_lookup = np.arange(id_order.shape[0])\n",
    "\n",
    "                # Step 3: Access the array using the sorted IDs\n",
    "                values = self._cache[sorted_ids]\n",
    "\n",
    "                # Step 4: Sort the values back to the original order\n",
    "                inverse_ids = np.empty_like(id_order)\n",
    "                inverse_ids[id_order] = id_lookup\n",
    "                values = values[inverse_ids]\n",
    "\n",
    "            return values\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._index)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        raise NotImplementedError(\"Item deletion is not implemented\")\n",
    "\n",
    "    def __setitem__(self, key: str, value: Optional[np.ndarray]):\n",
    "        assert not self.read_only\n",
    "        assert value is None or value.shape == self._base_shape\n",
    "\n",
    "        if not key in self:\n",
    "            self._add_to_cache(key, value)\n",
    "        elif key in self._new_cache:\n",
    "            self._new_cache[key] = value\n",
    "        else:\n",
    "            idx = self._index[key]\n",
    "            self._cache[idx] = value\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __bool__(self):\n",
    "        return True\n",
    "    \n",
    "    def flush(self):\n",
    "        if not self.read_only:\n",
    "            self.write_cache_to_disk()\n",
    "            \n",
    "    def close(self):\n",
    "        self.flush()\n",
    "        print(f\"Closing {self.tensor_file}\")\n",
    "        self._f.close()\n",
    "\n",
    "    def keys(self):\n",
    "        return self._index.keys()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.keys())\n",
    "    \n",
    "    def values(self):\n",
    "        return [self[key] for key in self]        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening ./cache_test/values.h5py\n"
     ]
    }
   ],
   "source": [
    "d1 = CachedTensorDict(path='.', name='cache_test', shape=(10,), read_only=False)\n",
    "d2 = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:12:26.900180595Z",
     "start_time": "2024-02-28T13:12:26.890925631Z"
    }
   },
   "id": "400272fd2bc8b15a",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#! rm -r cache_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:12:28.664158286Z",
     "start_time": "2024-02-28T13:12:28.654148795Z"
    }
   },
   "id": "4bae4b9e871cfe1f",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d1['c'] = np.arange(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:12:30.182777626Z",
     "start_time": "2024-02-28T13:12:30.172474989Z"
    }
   },
   "id": "71be831677562f95",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['a', 'b', 'c'])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:12:31.151466614Z",
     "start_time": "2024-02-28T13:12:31.132917144Z"
    }
   },
   "id": "b8ab7396571ab4bb",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1['c']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:12:31.934502690Z",
     "start_time": "2024-02-28T13:12:31.873423804Z"
    }
   },
   "id": "e68bf43f77098c4d",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<HDF5 dataset \"data\": shape (3, 10), type \"<f4\">"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1._cache"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:12:32.791015552Z",
     "start_time": "2024-02-28T13:12:32.775043825Z"
    }
   },
   "id": "3d4525ddee2f4a05",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d1.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:12:33.453776154Z",
     "start_time": "2024-02-28T13:12:33.431573062Z"
    }
   },
   "id": "bc0dca4b93b32cf",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<HDF5 dataset \"data\": shape (3, 10), type \"<f4\">"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1._cache"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:12:33.814716347Z",
     "start_time": "2024-02-28T13:12:33.801032832Z"
    }
   },
   "id": "f3b748a27e76dfb1",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing ./cache_test/values.h5py\n"
     ]
    }
   ],
   "source": [
    "del d1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:12:34.402008096Z",
     "start_time": "2024-02-28T13:12:34.391131846Z"
    }
   },
   "id": "79ae2cd84927dade",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a093f4dbe6e71620"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
